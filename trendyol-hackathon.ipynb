{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8673818-0b92-4402-87ca-51c2af4938f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1755611270905_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-26-112.eu-west-1.compute.internal:20888/proxy/application_1755611270905_0001/\" class=\"emr-proxy-link\" emr-resource=\"j-9XJDLSIITVK1\n",
       "\" application-id=\"application_1755611270905_0001\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-16-224.eu-west-1.compute.internal:8042/node/containerlogs/container_1755611270905_0001_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ed82d201b345329906f71d78e7162f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4069a3c7904df2845ca18bcdb698d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n",
      "\n",
      "Collecting pandas==1.5.3\n",
      "  Downloading pandas-1.5.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/lib/python3.9/site-packages (from pandas==1.5.3) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas==1.5.3) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./tmp/spark-a8deb592-c396-48bc-8063-496981e24fa3/lib64/python3.9/site-packages (from pandas==1.5.3) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.13.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Not uninstalling pandas at /usr/local/lib64/python3.9/site-packages, outside environment /mnt3/yarn/usercache/livy/appcache/application_1755611270905_0001/container_1755611270905_0001_01_000001/tmp/spark-a8deb592-c396-48bc-8063-496981e24fa3\n",
      "    Can't uninstall 'pandas'. No files were found to uninstall.\n",
      "Successfully installed pandas-1.5.3\n",
      "\n",
      "Collecting pyarrow==12.0.1\n",
      "  Downloading pyarrow-12.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in ./tmp/spark-a8deb592-c396-48bc-8063-496981e24fa3/lib64/python3.9/site-packages (from pyarrow==12.0.1) (1.26.4)\n",
      "Installing collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 20.0.0\n",
      "    Not uninstalling pyarrow at /usr/local/lib64/python3.9/site-packages, outside environment /mnt3/yarn/usercache/livy/appcache/application_1755611270905_0001/container_1755611270905_0001_01_000001/tmp/spark-a8deb592-c396-48bc-8063-496981e24fa3\n",
      "    Can't uninstall 'pyarrow'. No files were found to uninstall.\n",
      "Successfully installed pyarrow-12.0.1\n",
      "\n",
      "Collecting scikit-learn==1.3.2\n",
      "  Downloading scikit_learn-1.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "Collecting scipy>=1.5.0\n",
      "  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib64/python3.9/site-packages (from scikit-learn==1.3.2) (1.5.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in ./tmp/spark-a8deb592-c396-48bc-8063-496981e24fa3/lib64/python3.9/site-packages (from scikit-learn==1.3.2) (1.26.4)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.3.2 scipy-1.13.1 threadpoolctl-3.6.0\n",
      "\n",
      "Collecting lightgbm==4.3.0\n",
      "  Downloading lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n",
      "Requirement already satisfied: scipy in ./tmp/spark-a8deb592-c396-48bc-8063-496981e24fa3/lib64/python3.9/site-packages (from lightgbm==4.3.0) (1.13.1)\n",
      "Requirement already satisfied: numpy in ./tmp/spark-a8deb592-c396-48bc-8063-496981e24fa3/lib64/python3.9/site-packages (from lightgbm==4.3.0) (1.26.4)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.3.0\n",
      "\n",
      "Collecting xgboost==2.0.3\n",
      "  Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
      "Requirement already satisfied: numpy in ./tmp/spark-a8deb592-c396-48bc-8063-496981e24fa3/lib64/python3.9/site-packages (from xgboost==2.0.3) (1.26.4)\n",
      "Requirement already satisfied: scipy in ./tmp/spark-a8deb592-c396-48bc-8063-496981e24fa3/lib64/python3.9/site-packages (from xgboost==2.0.3) (1.13.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n",
      "\n",
      "Package              Version\n",
      "-------------------- ----------\n",
      "appdirs              1.4.4\n",
      "attrs                20.3.0\n",
      "aws-cfn-bootstrap    2.0\n",
      "awscli               2.25.0\n",
      "awscrt               0.23.8\n",
      "Babel                2.9.1\n",
      "beautifulsoup4       4.9.3\n",
      "boto                 2.49.0\n",
      "boto3                1.38.12\n",
      "botocore             1.38.12\n",
      "cffi                 1.14.5\n",
      "chardet              4.0.0\n",
      "chevron              0.13.1\n",
      "click                8.1.8\n",
      "cloud-init           22.2.2\n",
      "colorama             0.4.4\n",
      "configobj            5.0.6\n",
      "cryptography         36.0.1\n",
      "dbus-python          1.2.18\n",
      "distlib              0.3.1\n",
      "distro               1.5.0\n",
      "docutils             0.16\n",
      "ec2-hibinit-agent    1.0.8\n",
      "filelock             3.0.12\n",
      "gpg                  1.23.2\n",
      "idna                 2.10\n",
      "Jinja2               2.11.3\n",
      "jmespath             1.0.1\n",
      "joblib               1.5.0\n",
      "jsonpatch            1.21\n",
      "jsonpointer          2.0\n",
      "jsonschema           3.2.0\n",
      "libcomps             0.1.20\n",
      "lightgbm             4.3.0\n",
      "lockfile             0.12.2\n",
      "lxml                 5.4.0\n",
      "MarkupSafe           1.1.1\n",
      "matplotlib           3.9.4\n",
      "mysqlclient          1.4.2\n",
      "netifaces            0.10.6\n",
      "nltk                 3.9.1\n",
      "nose                 1.3.4\n",
      "numpy                1.26.4\n",
      "oauthlib             3.0.2\n",
      "packaging            21.3\n",
      "pandas               1.5.3\n",
      "pip                  21.3.1\n",
      "ply                  3.11\n",
      "prettytable          0.7.2\n",
      "prompt-toolkit       3.0.24\n",
      "py-dateutil          2.2\n",
      "pyarrow              12.0.1\n",
      "pycparser            2.20\n",
      "pyparsing            2.4.7\n",
      "pyrsistent           0.17.3\n",
      "pyserial             3.4\n",
      "PySocks              1.7.1\n",
      "python-daemon        2.3.0\n",
      "python-dateutil      2.8.1\n",
      "pytz                 2025.2\n",
      "PyYAML               5.4.1\n",
      "regex                2021.11.10\n",
      "release-notification 1.2\n",
      "requests             2.25.1\n",
      "rpm                  4.16.1.3\n",
      "ruamel.yaml          0.16.6\n",
      "ruamel.yaml.clib     0.1.2\n",
      "s3transfer           0.12.0\n",
      "scikit-learn         1.3.2\n",
      "scipy                1.13.1\n",
      "seaborn              0.13.2\n",
      "selinux              3.4\n",
      "sepolicy             3.4\n",
      "setools              4.4.1\n",
      "setuptools           59.6.0\n",
      "six                  1.13.0\n",
      "soupsieve            1.9.5\n",
      "support-info         1.0\n",
      "systemd-python       235\n",
      "threadpoolctl        3.6.0\n",
      "tqdm                 4.67.1\n",
      "urllib3              1.25.10\n",
      "virtualenv           20.4.0\n",
      "wcwidth              0.2.5\n",
      "wheel                0.37.1\n",
      "windmill             1.6\n",
      "xgboost              2.0.3\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas 2.2.3 requires tzdata>=2022.7, which is not installed.\n",
      "matplotlib 3.9.4 requires contourpy>=1.0.1, which is not installed.\n",
      "matplotlib 3.9.4 requires cycler>=0.10, which is not installed.\n",
      "matplotlib 3.9.4 requires fonttools>=4.22.0, which is not installed.\n",
      "matplotlib 3.9.4 requires importlib-resources>=3.2.0; python_version < \"3.10\", which is not installed.\n",
      "matplotlib 3.9.4 requires kiwisolver>=1.3.1, which is not installed.\n",
      "matplotlib 3.9.4 requires pillow>=8, which is not installed.\n",
      "pandas 2.2.3 requires python-dateutil>=2.8.2, but you have python-dateutil 2.8.1 which is incompatible.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag."
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "sc.install_pypi_package(\"numpy==1.26.4\")\n",
    "sc.install_pypi_package(\"pandas==1.5.3\")\n",
    "sc.install_pypi_package(\"pyarrow==12.0.1\")\n",
    "sc.install_pypi_package(\"scikit-learn==1.3.2\")\n",
    "\n",
    "# ihtiyacın varsa:\n",
    "sc.install_pypi_package(\"lightgbm==4.3.0\")\n",
    "sc.install_pypi_package(\"xgboost==2.0.3\")\n",
    "\n",
    "# Kuruldu mu kontrol et\n",
    "sc.list_packages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6518a9ea-6d6a-45df-bfb8-3fb609e01b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979592d665ae4736babed1c22ba4ee9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fdcc96ec3a0>"
     ]
    }
   ],
   "source": [
    "# PySpark oturumu: Hem PySpark kernelinde hem de EMR/Studio üzerinde güvenli.\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"TrendyolSearchRanking\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aebee2-ad89-41f9-b584-84393384dae0",
   "metadata": {},
   "source": [
    "## 2) Yol/Veri Kökü ve Güvenli Okuma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1368fea2-2840-4827-9f72-866d59dc4d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6e05880e9a4b9c8b1adc54d8b62978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sessions ? s3://melek-hackathon/raw/data/train_sessions.parquet\n",
      "test_sessions ? s3://melek-hackathon/raw/data/test_sessions.parquet\n",
      "content_meta ? s3://melek-hackathon/raw/data/content/metadata.parquet\n",
      "content_pricing ? s3://melek-hackathon/raw/data/content/price_rate_review_data.parquet\n",
      "content_search ? s3://melek-hackathon/raw/data/content/search_log.parquet\n",
      "content_sitewide ? s3://melek-hackathon/raw/data/content/sitewide_log.parquet\n",
      "content_top_terms ? s3://melek-hackathon/raw/data/content/top_terms_log.parquet\n",
      "user_meta ? s3://melek-hackathon/raw/data/user/metadata.parquet\n",
      "user_search ? s3://melek-hackathon/raw/data/user/search_log.parquet\n",
      "user_sitewide ? s3://melek-hackathon/raw/data/user/sitewide_log.parquet\n",
      "user_fashion_search ? s3://melek-hackathon/raw/data/user/fashion_search_log.parquet\n",
      "user_fashion_site ? s3://melek-hackathon/raw/data/user/fashion_sitewide_log.parquet\n",
      "term_search ? s3://melek-hackathon/raw/data/term/search_log.parquet"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = \"s3://melek-hackathon/raw/data\"\n",
    "\n",
    "from pyspark.sql.functions import col, lit, when, max as smax\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def p(path): \n",
    "    return f\"{DATA_ROOT.rstrip('/')}/{path.lstrip('/')}\"\n",
    "\n",
    "# Dosya yolları (katalog)\n",
    "PATHS = {\n",
    "    \"train_sessions\":      p(\"train_sessions.parquet\"),\n",
    "    \"test_sessions\":       p(\"test_sessions.parquet\"),\n",
    "    \"content_meta\":        p(\"content/metadata.parquet\"),\n",
    "    \"content_pricing\":     p(\"content/price_rate_review_data.parquet\"),\n",
    "    \"content_search\":      p(\"content/search_log.parquet\"),\n",
    "    \"content_sitewide\":    p(\"content/sitewide_log.parquet\"),\n",
    "    \"content_top_terms\":   p(\"content/top_terms_log.parquet\"),\n",
    "    \"user_meta\":           p(\"user/metadata.parquet\"),\n",
    "    \"user_search\":         p(\"user/search_log.parquet\"),\n",
    "    \"user_sitewide\":       p(\"user/sitewide_log.parquet\"),\n",
    "    \"user_fashion_search\": p(\"user/fashion_search_log.parquet\"),\n",
    "    \"user_fashion_site\":   p(\"user/fashion_sitewide_log.parquet\"),\n",
    "    \"term_search\":         p(\"term/search_log.parquet\"),\n",
    "}\n",
    "\n",
    "# Varlık kontrolü ve okunabilirlik\n",
    "for k,v in PATHS.items():\n",
    "    print(k, \"→\", v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b3993-042f-466a-bfa0-84c8bf63d05a",
   "metadata": {},
   "source": [
    "## 3) Veri Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b871da-680f-4e86-8234-62943a3761da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8461e1b2dee4fb68d443713f3d61a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 2773805  Test rows: 2988697"
     ]
    }
   ],
   "source": [
    "# Temel oturum verileri\n",
    "train = spark.read.parquet(PATHS[\"train_sessions\"])\n",
    "test  = spark.read.parquet(PATHS[\"test_sessions\"])\n",
    "\n",
    "# İçerik (ürün) tarafı\n",
    "cmeta   = spark.read.parquet(PATHS[\"content_meta\"])\n",
    "cprice  = spark.read.parquet(PATHS[\"content_pricing\"])\n",
    "csearch = spark.read.parquet(PATHS[\"content_search\"])\n",
    "csite   = spark.read.parquet(PATHS[\"content_sitewide\"])\n",
    "cterms  = spark.read.parquet(PATHS[\"content_top_terms\"])\n",
    "\n",
    "# Kullanıcı tarafı\n",
    "umeta   = spark.read.parquet(PATHS[\"user_meta\"])\n",
    "usearch = spark.read.parquet(PATHS[\"user_search\"])\n",
    "usite   = spark.read.parquet(PATHS[\"user_sitewide\"])\n",
    "ufash_s = spark.read.parquet(PATHS[\"user_fashion_search\"])\n",
    "ufash_w = spark.read.parquet(PATHS[\"user_fashion_site\"])\n",
    "\n",
    "# Terim tarafı\n",
    "tsearch = spark.read.parquet(PATHS[\"term_search\"])\n",
    "\n",
    "print(\"Train rows:\", train.count(), \" Test rows:\", test.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dffb317-ca9b-4239-8341-716110f68768",
   "metadata": {},
   "source": [
    "## 4) Hafif Temizlik ve Tip Uyumları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8acecf-bb35-4ab7-80dd-995bda3249b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa395e7537df4af9a1e8e3f1ff5c923b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, to_date, year, month, dayofweek, hour\n",
    "\n",
    "# Zaman ve tip dönüşümleri\n",
    "for df, ts_col in [(train, \"ts_hour\"), (test, \"ts_hour\")]:\n",
    "    if ts_col in df.columns:\n",
    "        df = df.withColumn(ts_col, to_timestamp(col(ts_col)))\n",
    "    # temel zaman kırılımları\n",
    "    df = df.withColumn(\"hour\", hour(col(ts_col)))\\\n",
    "           .withColumn(\"dow\", dayofweek(col(ts_col)))\\\n",
    "           .withColumn(\"month\", month(col(ts_col)))\n",
    "    if df is train:\n",
    "        train = df\n",
    "    else:\n",
    "        test = df\n",
    "\n",
    "# Eksik alan doldurma (özellikle string ve sayısal)\n",
    "def safe_fill(df):\n",
    "    str_cols  = [c for c,t in df.dtypes if t=='string']\n",
    "    num_cols  = [c for c,t in df.dtypes if t in ('int','bigint','double','float','long','short')]\n",
    "    df = df.fillna({c:\"__na__\" for c in str_cols}).fillna({c:0 for c in num_cols})\n",
    "    return df\n",
    "\n",
    "train = safe_fill(train)\n",
    "test  = safe_fill(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d20c3-621c-4068-a0f0-06eedee271f4",
   "metadata": {},
   "source": [
    "## 5) Zenginleştirme – Özellik (Feature) Türetme\n",
    "\n",
    "Fikir:\n",
    "\n",
    "- Ürün popülerliği / kalite sinyalleri (sitewide & search log oranları, puan/yorum, fiyat/indirim).\n",
    "- Kullanıcı demografisi + geçmiş etkileşim yoğunluğu.\n",
    "- Oturum saat/dow + terim ve ürün kategorilerinin “hashing” ile temsil edilmesi.\n",
    "- (content_id, search_term) eşleşmesi: content_top_terms’ten CTR sinyali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e15cd1-b9db-4f8b-9dd9-9e0649ee8756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd594a990fb4589884c603c13547f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, lit, when  # (opsiyonel, F.col vs. da kullanabilirsin)\n",
    "# NOT: row_number artık F.row_number olarak çağrılacak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f59b660-197f-4950-a1bf-8d5eeb84c8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bac58de527b4acabefbea5299cee0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_e cols: 48  test_e cols: 47"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ---- İçerik fiyat verisinde \"en güncel\" kaydı seç ----\n",
    "# Uygun zaman kolonu hangisi ise onu bul:\n",
    "time_col = next((c for c in [\"update_date\",\"updated_at\",\"event_time\",\"ts\",\"date\"]\n",
    "                 if c in cprice.columns), None)\n",
    "\n",
    "if time_col is not None:\n",
    "    w_content = Window.partitionBy(\"content_id_hashed\").orderBy(F.col(time_col).cast(\"timestamp\").desc())\n",
    "    cprice_last = (cprice\n",
    "                   .withColumn(\"rn\", F.row_number().over(w_content))\n",
    "                   .where(F.col(\"rn\")==1)\n",
    "                   .drop(\"rn\", time_col))\n",
    "else:\n",
    "    # Zaman kolonu yoksa direkt kullan (tek kayıt varsayımı / en güncel bilgi yok)\n",
    "    cprice_last = cprice\n",
    "\n",
    "# İndirim oranı\n",
    "cprice_last = cprice_last.withColumn(\n",
    "    \"discount_rate\",\n",
    "    F.when(F.col(\"original_price\") > 0,\n",
    "           (F.col(\"original_price\") - F.col(\"discounted_price\")) / F.col(\"original_price\"))\n",
    "     .otherwise(F.lit(0.0))\n",
    ")\n",
    "\n",
    "# Sitewide/search ortalamaları (ürün)\n",
    "def avg_all(df, key):\n",
    "    num_cols = [c for c,t in df.dtypes if c not in [key,\"date\",\"content_id_hashed\"] and t in ('double','float','int','bigint')]\n",
    "    agg = [smax(c).alias(f\"{c}_max\") if c.endswith(\"_count\") else F.avg(c).alias(f\"{c}_avg\") for c in num_cols]  # karışık ölçeklere güvenli yaklaşım\n",
    "    return df.groupBy(key).agg(*agg)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "csite_agg   = avg_all(csite, \"content_id_hashed\")\n",
    "csearch_agg = avg_all(csearch, \"content_id_hashed\")\n",
    "\n",
    "# (content_id, search_term) eşleşmesi: ilgili terim-ürün CTR\n",
    "cterms_agg = cterms.groupBy(\"content_id_hashed\",\"search_term_normalized\")\\\n",
    "                   .agg(F.avg(\"total_search_click\").alias(\"cterm_click_avg\"),\n",
    "                        F.avg(\"total_search_impression\").alias(\"cterm_impr_avg\"))\\\n",
    "                   .withColumn(\"cterm_ctr\", when(col(\"cterm_impr_avg\")>0, col(\"cterm_click_avg\")/col(\"cterm_impr_avg\")).otherwise(0.0))\n",
    "\n",
    "# ---- Kullanıcı bazlı özetler ----\n",
    "umeta2 = umeta.withColumn(\"user_age\", when(col(\"user_birth_year\")>1900, F.year(F.current_timestamp())-col(\"user_birth_year\")).otherwise(None))\\\n",
    "              .drop(\"user_birth_year\")\n",
    "\n",
    "usite_agg = avg_all(usite, \"user_id_hashed\")\\\n",
    "            .withColumnRenamed(\"total_click_avg\",\"user_total_click_avg\")\\\n",
    "            .withColumnRenamed(\"total_cart_avg\",\"user_total_cart_avg\")\\\n",
    "            .withColumnRenamed(\"total_fav_avg\",\"user_total_fav_avg\")\\\n",
    "            .withColumnRenamed(\"total_order_avg\",\"user_total_order_avg\")\n",
    "\n",
    "usearch_agg = avg_all(usearch, \"user_id_hashed\")\\\n",
    "            .withColumnRenamed(\"total_search_click_avg\",\"user_search_click_avg\")\\\n",
    "            .withColumnRenamed(\"total_search_impression_avg\",\"user_search_impr_avg\")\n",
    "\n",
    "# ---- Terim bazlı genel popülerlik ----\n",
    "tsearch_agg = tsearch.groupBy(\"search_term_normalized\")\\\n",
    "                     .agg(F.avg(\"total_search_click\").alias(\"term_click_avg\"),\n",
    "                          F.avg(\"total_search_impression\").alias(\"term_impr_avg\"))\\\n",
    "                     .withColumn(\"term_ctr\", when(col(\"term_impr_avg\")>0, col(\"term_click_avg\")/col(\"term_impr_avg\")).otherwise(0.0))\n",
    "\n",
    "# ---- Eğitim setini zenginleştir ----\n",
    "def enrich(sessions):\n",
    "    df = sessions.alias(\"s\")\\\n",
    "        .join(cmeta.alias(\"cm\"),   on=\"content_id_hashed\", how=\"left\")\\\n",
    "        .join(cprice_last.alias(\"cp\"), on=\"content_id_hashed\", how=\"left\")\\\n",
    "        .join(csite_agg.alias(\"cs\"),   on=\"content_id_hashed\", how=\"left\")\\\n",
    "        .join(csearch_agg.alias(\"cse\"),on=\"content_id_hashed\", how=\"left\")\\\n",
    "        .join(umeta2.alias(\"um\"),     on=\"user_id_hashed\", how=\"left\")\\\n",
    "        .join(usite_agg.alias(\"us\"),  on=\"user_id_hashed\", how=\"left\")\\\n",
    "        .join(usearch_agg.alias(\"ua\"), on=\"user_id_hashed\", how=\"left\")\\\n",
    "        .join(tsearch_agg.alias(\"ts\"), on=\"search_term_normalized\", how=\"left\")\\\n",
    "        .join(cterms_agg.alias(\"ct\"), on=[\"content_id_hashed\",\"search_term_normalized\"], how=\"left\")\n",
    "\n",
    "    # recency: ürün oluşturulma tarihi ile oturum farkı (gün)\n",
    "    df = df.withColumn(\"content_age_days\", \n",
    "                       (F.unix_timestamp(col(\"s.ts_hour\")) - F.unix_timestamp(col(\"cm.content_creation_date\")))/86400.0)\n",
    "\n",
    "    # güvenli doldurma\n",
    "    df = safe_fill(df)\n",
    "    return df\n",
    "\n",
    "train_e = enrich(train)\n",
    "test_e  = enrich(test)\n",
    "\n",
    "print(\"train_e cols:\", len(train_e.columns), \" test_e cols:\", len(test_e.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ae0c2b-e4dd-4bb1-b179-2a3c7f31e910",
   "metadata": {},
   "source": [
    "## 6) Özellik Dönüşümü (Hashing + Numerikler)\n",
    "\n",
    "Büyük kardinaliteli kategorik alanlar için FeatureHasher kullanıyoruz; metrikte önemli search_term_normalized, leaf_category_name, level1/2 + user_gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e38d97f5-541c-4d9f-bade-4b0881345594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20773594a0ef46e88952d3eb0f0e1a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(65557,[9412,13871,34802,35275,48684,65536,65537,65538,65539,65540,65541,65542,65543,65544,65545,65546,65547,65548,65549,65550,65551,65552,65553,65554,65556],[1.0,1.0,1.0,1.0,1.0,0.23322708543885565,0.0012863060456384144,9.863046356317874E-4,9.863046356317874E-4,4.666666666666667,8.62335492437761E-6,4.7907527357653384E-6,1.916301094306135E-6,8.468682050337184E-5,7.227192698525997E-6,1.739727636330785E-5,1.77942244471284E-6,4.589541120863194E-6,2.8744516414592027E-8,1.4372258207296016E-6,2.8744516414592033E-8,3.3970792126336035E-6,1.1870904581925203E-4,0.024637273546434694,165.70833333333334])|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import FeatureHasher, VectorAssembler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "hash_cols = [\"search_term_normalized\",\"leaf_category_name\",\"level1_category_name\",\"level2_category_name\",\"user_gender\"]\n",
    "num_cols = [\n",
    "    \"hour\",\"dow\",\"month\",\n",
    "    \"discount_rate\",\"original_price\",\"selling_price\",\"discounted_price\",\n",
    "    \"content_rate_avg\",\"content_rate_count\",\"content_review_count\",\"content_review_wth_media_count\",\n",
    "    \"total_click_avg\",\"total_cart_avg\",\"total_fav_avg\",\"total_order_avg\",\n",
    "    \"user_total_click_avg\",\"user_total_cart_avg\",\"user_total_fav_avg\",\"user_total_order_avg\",\n",
    "    \"user_search_click_avg\",\"user_search_impr_avg\",\n",
    "    \"term_ctr\",\"cterm_ctr\",\"content_age_days\"\n",
    "]\n",
    "# Bazı sütun adları join sonrası prefix almış olabilir; yoksa atlanır\n",
    "num_cols = [c for c in num_cols if c in train_e.columns]\n",
    "\n",
    "hasher = FeatureHasher(inputCols=[c for c in hash_cols if c in train_e.columns],\n",
    "                       outputCol=\"hashed_cat\", numFeatures=2**16)\n",
    "\n",
    "num_assembler = VectorAssembler(inputCols=num_cols, outputCol=\"num_vec\", handleInvalid=\"keep\")\n",
    "full_assembler = VectorAssembler(inputCols=[\"hashed_cat\",\"num_vec\"], outputCol=\"features\")\n",
    "\n",
    "fe_pipeline = Pipeline(stages=[hasher, num_assembler, full_assembler])\n",
    "fe_model = fe_pipeline.fit(train_e)\n",
    "train_f = fe_model.transform(train_e)\n",
    "test_f  = fe_model.transform(test_e)\n",
    "\n",
    "train_f.select(\"features\").limit(1).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f38eb36-40c2-4b8e-863e-f6c81b140638",
   "metadata": {},
   "source": [
    "## 7) Eğitim/Doğrulama Bölme (Zamana göre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31d79d79-901d-41c4-8c34-f4b7fd42705e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbc72cab4d94c1cbffec7854cbdc485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1796756 Valid: 977049"
     ]
    }
   ],
   "source": [
    "# 7) Eğitim/Doğrulama Bölme (Zamana göre) — DÜZELTİLMİŞ\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp, unix_timestamp, col, lit\n",
    "\n",
    "# ts_hour'ı garantiye al: (timestamp) -> (double: epoch seconds)\n",
    "train_f = train_f.withColumn(\"ts_hour\", to_timestamp(col(\"ts_hour\")))\n",
    "test_f  = test_f.withColumn(\"ts_hour\", to_timestamp(col(\"ts_hour\")))\n",
    "\n",
    "train_f = train_f.withColumn(\"ts_hour_num\", unix_timestamp(col(\"ts_hour\")).cast(\"double\"))\n",
    "test_f  = test_f.withColumn(\"ts_hour_num\",  unix_timestamp(col(\"ts_hour\")).cast(\"double\"))\n",
    "\n",
    "# Son %15'i valid olacak şekilde eşik (relativeError 0.01 gibi küçük bir değer olmalı)\n",
    "q = train_f.approxQuantile(\"ts_hour_num\", [0.85], 0.01)[0]\n",
    "\n",
    "# Bölme\n",
    "train_tr = train_f.where(col(\"ts_hour_num\") <  lit(q))\n",
    "train_va = train_f.where(col(\"ts_hour_num\") >= lit(q))\n",
    "\n",
    "print(\"Train:\", train_tr.count(), \"Valid:\", train_va.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e234889-2a16-4b22-9205-1dc485ba2034",
   "metadata": {},
   "source": [
    "## 8) Modellerin Eğitimi (Click & Order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4936180-efc5-45ad-ba67-48390f289f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693b231349aa45e0a28cb165f6cfdaf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid sample:\n",
      "+----------------------+-----------------+-------+------------------+-------+-------------------+\n",
      "|session_id            |content_id_hashed|clicked|p_click           |ordered|p_order            |\n",
      "+----------------------+-----------------+-------+------------------+-------+-------------------+\n",
      "|train_49f40a3ac0474cc4|fc95bba4457c8668 |0      |0.6297466734263741|0      |0.2904192710558313 |\n",
      "|train_f01bb9475dc09f31|d55efbf3a0d5e6f2 |0      |0.5432484478672712|0      |0.14393191655600202|\n",
      "|train_f2c19015490da3bc|495626dc3209b3b1 |0      |0.6337193178057551|0      |0.1890948270232825 |\n",
      "|train_fe8caf7f3fe6304e|2a79277a5073f4df |0      |0.5827684089692162|0      |0.20195026685314632|\n",
      "|train_cf1a1b1d7d639ac0|edad45fbfa885d54 |0      |0.552539113039862 |0      |0.17007590934965178|\n",
      "+----------------------+-----------------+-------+------------------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "# 8) Hafif Mod: Downsample + Logistic Regression (kernel düşmeden eğit)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# ---- VectorUDT -> sınıf-1 olasılığı (getItem yerine güvenli UDF) ----\n",
    "prob1_udf = F.udf(\n",
    "    lambda v: float(v[1]) if (v is not None and getattr(v, \"size\", 0) > 1)\n",
    "              else (float(v[0]) if v is not None else None),\n",
    "    DoubleType()\n",
    ")\n",
    "\n",
    "# ---- Parametreler: ağır gelirse oranları daha da düşür ----\n",
    "NEG_FRAC_CLICK = 0.02   # clicked=0 örnekleme oranı (örn. %2)\n",
    "NEG_FRAC_ORDER = 0.01   # ordered=0 örnekleme oranı (örn. %1)\n",
    "MAX_ITER_CLICK = 40\n",
    "MAX_ITER_ORDER = 60\n",
    "REG_CLICK = 0.05\n",
    "REG_ORDER = 0.05\n",
    "\n",
    "# ---- Etiketleri 0/1'e sabitle ----\n",
    "def binarize(df, colname):\n",
    "    return (df.fillna({colname: 0})\n",
    "              .withColumn(\n",
    "                  colname,\n",
    "                  F.when(F.col(colname).cast(\"string\").isin(\"1\",\"true\",\"True\"), 1).otherwise(0)\n",
    "              )\n",
    "              .withColumn(colname, F.col(colname).cast(\"int\")))\n",
    "\n",
    "for nm in [\"train_tr\",\"train_va\"]:\n",
    "    df = globals()[nm]\n",
    "    df = binarize(df, \"clicked\")\n",
    "    df = binarize(df, \"ordered\")\n",
    "    globals()[nm] = df\n",
    "\n",
    "# ---- Downsample: tüm pozitifler + az sayıda negatif ----\n",
    "click_pos = train_tr.filter(\"clicked = 1\")\n",
    "click_neg = train_tr.filter(\"clicked = 0\").sample(False, NEG_FRAC_CLICK, seed=42)\n",
    "train_tr_click = click_pos.unionByName(click_neg)\n",
    "\n",
    "order_pos = train_tr.filter(\"ordered = 1\")\n",
    "order_neg = train_tr.filter(\"ordered = 0\").sample(False, NEG_FRAC_ORDER, seed=42)\n",
    "train_tr_order = order_pos.unionByName(order_neg)\n",
    "\n",
    "# ---- Modeller: LR hızlı ve kararlı ----\n",
    "lr_click = LogisticRegression(\n",
    "    featuresCol=\"features\", labelCol=\"clicked\",\n",
    "    predictionCol=\"click_pred\", probabilityCol=\"click_prob\",\n",
    "    maxIter=MAX_ITER_CLICK, regParam=REG_CLICK, elasticNetParam=0.2\n",
    ")\n",
    "lr_order = LogisticRegression(\n",
    "    featuresCol=\"features\", labelCol=\"ordered\",\n",
    "    predictionCol=\"order_pred\", probabilityCol=\"order_prob\",\n",
    "    maxIter=MAX_ITER_ORDER, regParam=REG_ORDER, elasticNetParam=0.3\n",
    ")\n",
    "\n",
    "# ---- Eğitim ----\n",
    "model_click = lr_click.fit(train_tr_click)\n",
    "model_order = lr_order.fit(train_tr_order)\n",
    "\n",
    "# ---- Valid üstünde tahminler (DİKKAT: 'train_va_o' yok; 'train_va' kullanılacak) ----\n",
    "va_click = (model_click.transform(train_va)\n",
    "            .select(\"session_id\",\"content_id_hashed\",\"clicked\",\"click_prob\")\n",
    "            .withColumn(\"p_click\", prob1_udf(F.col(\"click_prob\")))\n",
    "            .drop(\"click_prob\"))\n",
    "\n",
    "va_order = (model_order.transform(train_va)\n",
    "            .select(\"session_id\",\"content_id_hashed\",\"ordered\",\"order_prob\")\n",
    "            .withColumn(\"p_order\", prob1_udf(F.col(\"order_prob\")))\n",
    "            .drop(\"order_prob\"))\n",
    "\n",
    "# ---- Birleştir ----\n",
    "valid_pred = (va_click\n",
    "              .join(va_order, [\"session_id\",\"content_id_hashed\"], \"inner\"))\n",
    "\n",
    "print(\"valid sample:\")\n",
    "valid_pred.limit(5).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6569e7d-0ced-4fc3-b6e4-5ea9ba6073aa",
   "metadata": {},
   "source": [
    "## 9) Oturum bazlı AUC ve final valid skoru (pandas’sız, hafif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66c0c899-f447-4f51-a202-85465b11526f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4c5bb0cb4b42c7803e3921861b24e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ? Click AUC=0.5647  Order AUC=0.6280  Final=0.6090"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "# valid_pred: (session_id, content_id_hashed, clicked, ordered, p_click, p_order)\n",
    "# yoksa 8. adımdan önceki hücreyi çalıştır.\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# --- AUC hesaplayıcı (ties destekli, saf Python) ---\n",
    "def _auc_from_lists(probs, labels):\n",
    "    if probs is None or labels is None:\n",
    "        return None\n",
    "    n = len(probs)\n",
    "    if n == 0:\n",
    "        return None\n",
    "    n1 = sum(1 for y in labels if int(y) == 1)\n",
    "    n0 = n - n1\n",
    "    if n1 == 0 or n0 == 0:\n",
    "        return None\n",
    "\n",
    "    # rank with average ties\n",
    "    idx = list(range(n))\n",
    "    idx.sort(key=lambda i: float(probs[i]))\n",
    "    ranks = [0.0] * n\n",
    "    r = 1\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        j = i\n",
    "        pi = probs[idx[i]]\n",
    "        while j + 1 < n and probs[idx[j + 1]] == pi:\n",
    "            j += 1\n",
    "        avg_rank = (r + (j + 1)) / 2.0\n",
    "        for k in range(i, j + 1):\n",
    "            ranks[idx[k]] = avg_rank\n",
    "        r = j + 2\n",
    "        i = j + 1\n",
    "\n",
    "    sum_pos = sum(ranks[i] for i, y in enumerate(labels) if int(y) == 1)\n",
    "    return float((sum_pos - n1 * (n1 + 1) / 2.0) / (n1 * n0))\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "auc_udf = udf(_auc_from_lists, DoubleType())\n",
    "\n",
    "# Click AUC: sadece pozitif click içeren oturumlar\n",
    "click_groups = (valid_pred\n",
    "    .groupBy(\"session_id\")\n",
    "    .agg(F.collect_list(\"p_click\").alias(\"p\"), F.collect_list(\"clicked\").alias(\"y\")))\n",
    "click_auc_df = click_groups.withColumn(\"auc_click\", auc_udf(\"p\",\"y\")).where(F.col(\"auc_click\").isNotNull())\n",
    "auc_click = click_auc_df.agg(F.avg(\"auc_click\").alias(\"m\")).first()[\"m\"]\n",
    "\n",
    "# Order AUC: sadece pozitif order içeren oturumlar\n",
    "order_groups = (valid_pred\n",
    "    .groupBy(\"session_id\")\n",
    "    .agg(F.collect_list(\"p_order\").alias(\"p\"), F.collect_list(\"ordered\").alias(\"y\")))\n",
    "order_auc_df = order_groups.withColumn(\"auc_order\", auc_udf(\"p\",\"y\")).where(F.col(\"auc_order\").isNotNull())\n",
    "auc_order = order_auc_df.agg(F.avg(\"auc_order\").alias(\"m\")).first()[\"m\"]\n",
    "\n",
    "final_valid_score = 0.3*float(auc_click or 0.0) + 0.7*float(auc_order or 0.0)\n",
    "print(f\"Validation → Click AUC={auc_click:.4f}  Order AUC={auc_order:.4f}  Final={final_valid_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15061b8-cac3-4710-8ac6-78da746f3450",
   "metadata": {},
   "source": [
    "## 10) Full-train ile yeniden eğitim (hafif – downsample + LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7cca151-50e1-4c18-af8e-7e1a7b8c2810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603884a3397f4e6faabfb542ade02a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full-train refit done."
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "# Aynı downsample stratejisiyle tüm train üstünde yeniden fit (test için daha iyi genelleme)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Parametreler (gerekirse değiştir)\n",
    "NEG_FRAC_CLICK = 0.02\n",
    "NEG_FRAC_ORDER = 0.01\n",
    "MAX_ITER_CLICK = 50\n",
    "MAX_ITER_ORDER = 70\n",
    "REG_CLICK = 0.05\n",
    "REG_ORDER = 0.05\n",
    "\n",
    "# Etiketleri garanti et (8. adımda yapıldıysa idempotent)\n",
    "def binarize(df, colname):\n",
    "    return (df.fillna({colname: 0})\n",
    "              .withColumn(colname, F.when(F.col(colname).cast(\"string\").isin(\"1\",\"true\",\"True\"),1).otherwise(0).cast(\"int\")))\n",
    "\n",
    "train_f2 = binarize(train_f, \"clicked\")\n",
    "train_f2 = binarize(train_f2, \"ordered\")\n",
    "\n",
    "# Downsample + union\n",
    "click_pos_all = train_f2.filter(\"clicked = 1\")\n",
    "click_neg_all = train_f2.filter(\"clicked = 0\").sample(False, NEG_FRAC_CLICK, seed=13)\n",
    "train_all_click = click_pos_all.unionByName(click_neg_all)\n",
    "\n",
    "order_pos_all = train_f2.filter(\"ordered = 1\")\n",
    "order_neg_all = train_f2.filter(\"ordered = 0\").sample(False, NEG_FRAC_ORDER, seed=13)\n",
    "train_all_order = order_pos_all.unionByName(order_neg_all)\n",
    "\n",
    "# Modeller\n",
    "lr_click_full = LogisticRegression(\n",
    "    featuresCol=\"features\", labelCol=\"clicked\",\n",
    "    predictionCol=\"click_pred\", probabilityCol=\"click_prob\",\n",
    "    maxIter=MAX_ITER_CLICK, regParam=REG_CLICK, elasticNetParam=0.2\n",
    ")\n",
    "lr_order_full = LogisticRegression(\n",
    "    featuresCol=\"features\", labelCol=\"ordered\",\n",
    "    predictionCol=\"order_pred\", probabilityCol=\"order_prob\",\n",
    "    maxIter=MAX_ITER_ORDER, regParam=REG_ORDER, elasticNetParam=0.3\n",
    ")\n",
    "\n",
    "model_click_full = lr_click_full.fit(train_all_click)\n",
    "model_order_full = lr_order_full.fit(train_all_order)\n",
    "\n",
    "print(\"Full-train refit done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e50f51-f456-4c75-bb99-26773588d351",
   "metadata": {},
   "source": [
    "## 11) Test tahmini ve oturum içi sıralama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11fa74b7-33ef-44b5-9696-2206d49d447c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cff1888ad441da9e86c30268dbe038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|session_id           |prediction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|test_004bfff42383f526|fe62b307a0c6f56d 6684e890885b30a1 d065bbef7415549d e578efd8bbb92b81 f1ea7ff813ac6caf bd8c5257a97a5038 5ada87d332ee9b1d a2815000597225a9 1f7071da530f761c 6de30df3492e9863 71209ed7d4e04537 d74c52a7bc31bdca fdc803630e8f4f14 fa73eecdc3cce853 0fe2d03480817d43 dd3ff75d564e1dde 5b568fbb2515dd14 b000181752ed70ff fe58ee03a26e0cd1 777e4bccef222dd2 c5c65331eed31091 55301449144e020c 04d36d9df52d78d1 36fb19a9141f8ee4 606db84ab2190ea6 d835b44819ed55c8 5f98e8f28889c709 ae4091b2da077992 4001a0c61ebb28f7 b861caac8cd39db6 530e68b09816d5ea 9952bd69acbb6fb4 e7b4ff2f7263fdbb 80e90f6557963290 d94273917c68656e e7592ae1352b3b46 2f952f97ccbb7a70 5774f0f396c7ed33 b710c17e2b3613ad 316f53b5fe168df2 2f410b6615ad89fa 53b8e2078b86aa9f e3b1ed96ca0e32b6 9d90fbbb528a4ab2 dee173c1dfc5dbcb 1ea6f028539fe32d 1976cfc6ce333a4d 66f1192ab806aedc 8616053c9eac07e4 57cddc09373f7ee6 58ea207cfca6e9d0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|test_00a839781fc90905|c086bb544ace425d e9d87b777b0c1c84 96236301b700714d 306a29e07e06fb58 a661a1503c7eb8c7 371cbb68ac378691 2eebef3db262cb8d 9fafb9a47ef2582d 7d3b740dda7abfc1 fd8c8d41461f310d 3d886afe1845603d 6831935c80d528eb 58575067e5df7844 bd7874fd651496b4 df6844c9913d0934 51053cc5b335f3a7 7e119edf088c9374 9e7043c068bc391c 7de4fcb3e98376df 02d8e28747438cb0 b4e9d4aa93110709 ec20a34aa4a5ec6e 579d5e382d190321 8fdf194c9121b600 27fdd944af08733c f0573c8fcc69510c 560fe0d528a15b7d e38855ef46da8402 37af61561cf5b9fa 4f1f58a0c695a77b 831b904ace6a2376 a02caa6b0aa6afce 418886378e12d265 a49dd99e993dcb8a b880cb10e4b3de46 04a52e5818a6449a 6cb2aee47937a91b f645453791cc3b2c 5e9ac78a4b3d9a37 fbe6ea5982f3d511 13f009ffdcfeff20 97a00dcef3cbc72c bdd6e6457704ab65 78241642240fe612 09d4f58a06925aac 8e7f4768c2907f9d efc39964369fab7d a278fdbfd51e20f5 7d010491f44d6fa4 8fa071b8e1ddd1f4 e612dc725db4f04b f8a60e898ba4648e 9a9b8b9483d7c427 9db13d2168256fb6 85198919aa0381ca 6ec590b7f428ff8c c20642de826ac498 56194f4b1fd5cc53 ba85c0809f4654f9 0ac60c302519bfe3 9bfa8c9a5c74360f 0482b64400daf55a ff3edb1f0087ab28 055e2d22dfa8ee49 d7e0c008eb1c7aa1 b3806e5eb02d7c6c ae14edda2818688a 15bb30dc1dd1a3fe a6004875a2c12614 72318304bf038a4b 72de34ce1f0df953 a86fd9bd2f202d6e 6f7d2656c1b540f0 fe3eef0185406d60 7f3386f090cf6353 af7c3a53baf7f5f2 02087472981087a7 df7e761f68c00612 c9fcb4e2cf773156 0fe092b3fdac8110 0583b5fa1fde489a 1ed68150c4ab1fca 819139510b47fbe8 04d4b5e88dcc45ae e545cce559e72669 70cd9698e43f9a5d f76beb0570f5e06c a666a1d4bd0d1d3f 0cc55f266e327ebc 1e7f7508515349ad c0a634fd0418c265 fac8544911e0ee49 d642beef666c9cb4 11cce6d57eef1826 3d798ac186312a19 e2f9277d91854b04 de0ef37421f62c35 b57bbe6828f66dec 5a33f1758eaa5231 3ce07702591abaf4 8cde1967859669d4 4844dff5e5575b8a 4562028e16553819 2cbd44ce53916933 666d3e63c26fc495 f60f83d0d3d00eef 62a43087859f7c29 c8da9d5c1e201396 04ada5d7bdc0f40b 6fcca91a12da95ea 261935eedc602103 836edfa6a9e51623 8fb90527ac2d5dea|\n",
      "|test_00f3978adfd37a40|a599792e3f34f2f6 4872f681cdfd2abf 1933dd8443a99c4d 0d42448e48815d54 f799b9b83960d60e f1c3d7ad11ad2f30 905e9c15a57bc4b0 407c910ea00d9ab3 d5181bfcc0a5505c cf9251ee8c3a6c56 fc6bc80e494b34e4 92b692521ecd5a0a 11978484c825ed09 c55530457861388d d576f201ae097917 c3518a750cb94a7e b8aa56c2c771bb52 fdc731a3839ab540 cf9ec9da3a51178b 3b6c3cb7ffab4710 b2dc3af63d7f2415 718c6f177d91825f db6418b43f31ab69 82fa46ba526940bf 6876002e0e7faf05 aead355c9e98dd18 34fbd73c673a851e a9746001629a5a7f c3f9dcc08f0216a6 fb4aea426754059b 77c6e4d86dce962f 4cbb2ac60561cd8e ae71c3ce88ec335e ae1173e5993a9cb5 14472a55c4c392ee 226d8b1071412127 e58eb9cdcb22a319 70cf55109c24d548 10a067eb0e1e7026 88e02b9a509cf916 c88c8f1ce5b6b173 f52e2a3e170faf85 8e07359a4df62712 25892d3a93694abe d7c69536047d0c38 4e04719a78682bbb f458575d005c5a95 2c0d189f3ccbd8e2 69b1e5e4a08ca9b2 16c74a75d4898005 34c1b83f9454a806 3b371ef82378f84b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Test sessions: 18589"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Sınıf-1 olasılık çıkarıcı (vektörden)\n",
    "from pyspark.sql.types import DoubleType\n",
    "prob1_udf = F.udf(\n",
    "    lambda v: float(v[1]) if (v is not None and getattr(v, \"size\", 0) > 1)\n",
    "              else (float(v[0]) if v is not None else None),\n",
    "    DoubleType()\n",
    ")\n",
    "\n",
    "# Tahminler\n",
    "t_click = (model_click_full.transform(test_f)\n",
    "           .select(\"session_id\",\"content_id_hashed\",\"click_prob\")\n",
    "           .withColumn(\"p_click\", prob1_udf(F.col(\"click_prob\")))\n",
    "           .drop(\"click_prob\"))\n",
    "\n",
    "t_order = (model_order_full.transform(test_f)\n",
    "           .select(\"session_id\",\"content_id_hashed\",\"order_prob\")\n",
    "           .withColumn(\"p_order\", prob1_udf(F.col(\"order_prob\")))\n",
    "           .drop(\"order_prob\"))\n",
    "\n",
    "test_pred = (t_click.join(t_order, [\"session_id\",\"content_id_hashed\"], \"inner\")\n",
    "                    .withColumn(\"final_score\", 0.3*F.col(\"p_click\") + 0.7*F.col(\"p_order\")))\n",
    "\n",
    "# Gruplayıp skorla azalan sırala (pandas'sız)\n",
    "def _join_sorted(cids, scores):\n",
    "    pairs = sorted(zip(scores, cids), key=lambda x: float(x[0]) if x[0] is not None else -1e9, reverse=True)\n",
    "    return \" \".join([str(cid) for _, cid in pairs])\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "join_sorted_udf = udf(_join_sorted, StringType())\n",
    "\n",
    "pred_df = (test_pred.groupBy(\"session_id\")\n",
    "           .agg(join_sorted_udf(F.collect_list(\"content_id_hashed\"),\n",
    "                                F.collect_list(\"final_score\")).alias(\"prediction\")))\n",
    "\n",
    "pred_df.limit(3).show(truncate=False)\n",
    "print(\"Test sessions:\", pred_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3798a0d-84dc-49c2-a7f4-5ab4addccb31",
   "metadata": {},
   "source": [
    "## 12) Submission yazımı (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28c4ac97-fbe8-4903-8058-750c8cbc3874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b8b386373a45699f36ef10288a56cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission yaz?ld? ? s3://melek-hackathon/submissions/submission.csv"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "# 12) SUBMISSION'u S3'e TEK CSV OLARAK YAZ\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "BASE  = \"s3://melek-hackathon/submissions\"\n",
    "TMP   = f\"{BASE}/_tmp_submission\"\n",
    "FINAL = f\"{BASE}/submission.csv\"\n",
    "\n",
    "# 1) Geçici klasöre tek parça CSV yaz\n",
    "(pred_df\n",
    " .orderBy(\"session_id\")\n",
    " .coalesce(1)\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"header\", True)\n",
    " .csv(TMP))\n",
    "\n",
    "# 2) part-*.csv'yi 'submission.csv' olarak yeniden adlandır (aynı bucket içinde rename)\n",
    "hconf = sc._jsc.hadoopConfiguration()\n",
    "Path  = sc._jvm.org.apache.hadoop.fs.Path\n",
    "fs    = Path(BASE).getFileSystem(hconf)\n",
    "\n",
    "# part dosyasını bul\n",
    "for status in fs.listStatus(Path(TMP)):\n",
    "    name = status.getPath().getName()\n",
    "    if name.startswith(\"part-\") and name.endswith(\".csv\"):\n",
    "        part_path = status.getPath()\n",
    "        break\n",
    "else:\n",
    "    raise Exception(\"part-*.csv bulunamadı.\")\n",
    "\n",
    "# varsa eski 'submission.csv'yi sil\n",
    "final_path = Path(FINAL)\n",
    "if fs.exists(final_path):\n",
    "    fs.delete(final_path, True)\n",
    "\n",
    "# rename → tek CSV dosyası\n",
    "fs.rename(part_path, final_path)\n",
    "\n",
    "# geçiciyi temizle\n",
    "fs.delete(Path(TMP), True)\n",
    "\n",
    "print(\"Submission yazıldı →\", FINAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa035fa-af10-4bf8-99a2-c0d5dfe91497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
